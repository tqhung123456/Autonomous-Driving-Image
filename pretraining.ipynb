{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tgx4AMZo8anP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.29.1\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "print(f\"{gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U_1ZNBum8ane"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iaz2Szrl8anx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dS9lr70R9eU9"
      },
      "outputs": [],
      "source": [
        "# Example for continuous actions\n",
        "# env_id = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "# Example for discrete actions\n",
        "env_id = \"Pendulum-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eh88d4oR8an6"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_id, render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKCgHCc_8aoB"
      },
      "source": [
        "## Train Expert Model\n",
        "\n",
        "We create an expert RL agent and let it learn to solve a task by interacting with the evironment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EkmIST0r8aoC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "# ppo_expert = TD3(\"MlpPolicy\", env_id, verbose=1)\n",
        "# ppo_expert.learn(total_timesteps=1e5)\n",
        "# ppo_expert.save(\"ppo_expert\")\n",
        "ppo_expert = TD3.load(\"ppo_expert\", env)\n",
        "# ppo_expert.learn(total_timesteps=1e5)\n",
        "# ppo_expert.save(\"ppo_expert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlyTfGAQ_Az1"
      },
      "source": [
        "check the performance of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(env_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-_rVEjC0_AQa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = -289.604587072134 +/- 104.41954344153768\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Kv6v_V8aoJ"
      },
      "source": [
        "## Create Student\n",
        "\n",
        "We also create a student RL agent, which will later be trained with the expert dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fLdLPUeC8aoL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'Pendulum-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "student = TD3(\"MlpPolicy\", env_id, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sdW8_41-OcXn"
      },
      "outputs": [],
      "source": [
        "# only valid for continuous actions\n",
        "# sac_student = SAC('MlpPolicy', env_id, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3GuNxcU8aoT"
      },
      "source": [
        "\n",
        "We now let our expert interact with the environment (except we already have expert data) and store resultant expert observations and actions to build an expert dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "emodyZDW8aoU"
      },
      "outputs": [],
      "source": [
        "num_interactions = int(4e4 + 1)\n",
        "# num_interactions = int(510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_returns(rewards, dones, discount_factor=0.99):\n",
        "    returns = []\n",
        "    G = 0  # Initialize the return for the current episode\n",
        "\n",
        "    # Iterate backwards through rewards\n",
        "    for reward, done in zip(reversed(rewards), reversed(dones)):\n",
        "        if done:\n",
        "            G = 0  # Reset the return at the end of each episode\n",
        "        G = reward + discount_factor * G  # Update the return\n",
        "        returns.insert(0, G)  # Insert the return at the beginning of the list\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "F3I_2s808aoZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40001/40001 [00:37<00:00, 1076.15it/s]\n"
          ]
        }
      ],
      "source": [
        "expert_observations = np.empty((num_interactions,) + env.observation_space.shape, dtype=np.float32)\n",
        "expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],), dtype=np.float32)\n",
        "expert_rewards = np.empty((num_interactions,) + (1,), dtype=np.float32)\n",
        "expert_dones = np.empty((num_interactions,) + (1,), dtype=np.float32)\n",
        "expert_next_observations = np.empty((num_interactions,) + env.observation_space.shape, dtype=np.float32)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "\n",
        "for i in tqdm(range(num_interactions)):\n",
        "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
        "    expert_observations[i] = obs\n",
        "    expert_actions[i] = action\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    expert_rewards[i] = reward\n",
        "    expert_dones[i] = terminated * (1 - truncated)\n",
        "    if terminated or truncated:\n",
        "        obs, _ = env.reset()\n",
        "expert_next_observations = expert_observations[1:].copy()\n",
        "expert_observations = expert_observations[:-1]\n",
        "expert_actions = expert_actions[:-1]\n",
        "expert_rewards = expert_rewards[:-1]\n",
        "expert_dones = expert_dones[:-1]\n",
        "\n",
        "# expert_returns = calculate_returns(expert_returns, expert_dones)\n",
        "# for idx, done in enumerate(expert_dones):\n",
        "#     if done:\n",
        "#         print(idx)\n",
        "#         break\n",
        "# print(expert_returns[:idx+3])\n",
        "\n",
        "np.savez_compressed(\n",
        "    \"expert_data\",\n",
        "    expert_observations=expert_observations,\n",
        "    expert_actions=expert_actions,\n",
        "    expert_rewards=expert_rewards,\n",
        "    expert_dones=expert_dones,\n",
        "    expert_next_observations=expert_next_observations,\n",
        ")\n",
        "\n",
        "# # Load the data\n",
        "# data = np.load(\"expert_data.npz\")\n",
        "\n",
        "# # Access the saved arrays using their keys\n",
        "# expert_observations = data['expert_observations']\n",
        "# expert_actions = data['expert_actions']\n",
        "# expert_rewards = data['expert_rewards']\n",
        "# expert_dones = data['expert_dones']\n",
        "# expert_next_observations = data['expert_next_observations']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32\n",
            "float32\n",
            "float32\n",
            "float32\n",
            "float32\n"
          ]
        }
      ],
      "source": [
        "print(expert_observations[0].dtype)\n",
        "print(expert_next_observations[0].dtype)\n",
        "print(expert_actions[0].dtype)\n",
        "print(expert_rewards[0].dtype)\n",
        "print(expert_dones[0].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toKEQE9i8aof"
      },
      "source": [
        "\n",
        "\n",
        "- To seamlessly use PyTorch in the training process, we subclass an `ExpertDataset` from PyTorch's base `Dataset`.\n",
        "- Note that we initialize the dataset with the previously generated expert observations and actions.\n",
        "- We further implement Python's `__getitem__` and `__len__` magic functions to allow PyTorch's dataset-handling to access arbitrary rows in the dataset and inform it about the length of the dataset.\n",
        "- For more information about PyTorch's datasets, you can read: https://pytorch.org/docs/stable/data.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qT72bR1i8aog"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xUetr5vl8aom"
      },
      "outputs": [],
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        expert_observations,\n",
        "        expert_actions,\n",
        "        expert_rewards,\n",
        "        expert_dones,\n",
        "        expert_next_observations,\n",
        "    ):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "        self.rewards = expert_rewards\n",
        "        self.dones = expert_dones\n",
        "        self.next_observations = expert_next_observations\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            self.observations[index],\n",
        "            self.actions[index],\n",
        "            self.rewards[index],\n",
        "            self.dones[index],\n",
        "            self.next_observations[index],\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bNAhXp8aor"
      },
      "source": [
        "\n",
        "\n",
        "We now instantiate the `ExpertDataSet` and split it into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lIdT-zMV8aot"
      },
      "outputs": [],
      "source": [
        "expert_dataset = ExpertDataSet(\n",
        "    expert_observations,\n",
        "    expert_actions,\n",
        "    expert_rewards,\n",
        "    expert_dones,\n",
        "    expert_next_observations,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "train_size = int(1 * len(expert_dataset))\n",
        "\n",
        "\n",
        "\n",
        "test_size = len(expert_dataset) - train_size\n",
        "\n",
        "\n",
        "\n",
        "train_expert_dataset, test_expert_dataset = random_split(\n",
        "    expert_dataset, [train_size, test_size]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_LgmtFFq8aox"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_expert_dataset:  0\n",
            "train_expert_dataset:  40000\n"
          ]
        }
      ],
      "source": [
        "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
        "print(\"train_expert_dataset: \", len(train_expert_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v8PhG2r8ao4"
      },
      "source": [
        "\n",
        "\n",
        "NOTE: The supervised learning section of this code is adapted from: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "1. We extract the policy network of our RL student agent.\n",
        "2. We load the (labeled) expert dataset containing expert observations as inputs and expert actions as targets.\n",
        "3. We perform supervised learning, that is, we adjust the policy network's parameters such that given expert observations as inputs to the network, its outputs match the targets (expert actions).\n",
        "By training the policy network in this way the corresponding RL student agent is taught to behave like the expert agent that was used to created the expert dataset (Behavior Cloning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VwUhCTGU8ao5"
      },
      "outputs": [],
      "source": [
        "# def pretrain_agent(\n",
        "#     student,\n",
        "#     batch_size=64,\n",
        "#     epochs=1000,\n",
        "#     learning_rate=0.001,\n",
        "#     log_interval=100,\n",
        "#     no_cuda=False,\n",
        "#     seed=1,\n",
        "#     test_batch_size=64,\n",
        "# ):\n",
        "#     use_cuda = not no_cuda and th.cuda.is_available()\n",
        "#     th.manual_seed(seed)\n",
        "#     device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "#     kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "#     if isinstance(env.action_space, gym.spaces.Box):\n",
        "#         criterion = nn.MSELoss()\n",
        "#     else:\n",
        "#         criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#     # Extract initial policy\n",
        "#     actor = student.policy.actor.to(device)\n",
        "#     critic = student.policy.critic.to(device)\n",
        "\n",
        "#     def train(actor, critic, device, train_loader, actor_optimizer, critic_optimizer):\n",
        "#         actor.train()\n",
        "#         critic.train()\n",
        "\n",
        "#         for batch_idx, (data, target_action, target_return) in enumerate(train_loader):\n",
        "#             data, target_action, target_return = (\n",
        "#                 data.to(device),\n",
        "#                 target_action.to(device),\n",
        "#                 target_return.to(device),\n",
        "#             )\n",
        "\n",
        "#             target_action = target_action.float()\n",
        "#             target_return = target_return.float()\n",
        "\n",
        "#             action = actor(data)\n",
        "#             actor_loss = criterion(action, target_action)\n",
        "\n",
        "#             actor_optimizer.zero_grad()\n",
        "#             actor_loss.backward()\n",
        "#             actor_optimizer.step()\n",
        "\n",
        "#             current_returns = critic(data, target_action)\n",
        "#             critic_loss = sum(\n",
        "#                 F.mse_loss(current_return, target_return)\n",
        "#                 for current_return in current_returns\n",
        "#             )\n",
        "\n",
        "#             critic_optimizer.zero_grad()\n",
        "#             critic_loss.backward()\n",
        "#             critic_optimizer.step()\n",
        "\n",
        "#             if batch_idx % log_interval == 0:\n",
        "#                 print(\n",
        "#                     \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tActor Loss: {:.6f}\\tCritic Loss: {:.6f}\".format(\n",
        "#                         epoch,\n",
        "#                         batch_idx * len(data),\n",
        "#                         len(train_loader.dataset),\n",
        "#                         100.0 * batch_idx / len(train_loader),\n",
        "#                         actor_loss.item(),\n",
        "#                         critic_loss.item(),\n",
        "#                     )\n",
        "#                 )\n",
        "\n",
        "#     def test(actor, critic, device, test_loader):\n",
        "#         actor.eval()\n",
        "#         critic.eval()\n",
        "#         actor_loss = 0\n",
        "#         critic_loss = 0\n",
        "#         with th.no_grad():\n",
        "#             for data, target_action, target_return in test_loader:\n",
        "#                 data, target_action, target_return = (\n",
        "#                     data.to(device),\n",
        "#                     target_action.to(device),\n",
        "#                     target_return.to(device),\n",
        "#                 )\n",
        "\n",
        "#                 target_action = target_action.float()\n",
        "#                 target_return = target_return.float()\n",
        "\n",
        "#                 action = actor(data)\n",
        "#                 actor_loss += criterion(action, target_action)\n",
        "#                 current_returns = critic(data, target_action)\n",
        "#                 critic_loss += sum(\n",
        "#                     F.mse_loss(current_return, target_return)\n",
        "#                     for current_return in current_returns\n",
        "#                 )\n",
        "\n",
        "#         actor_loss /= len(test_loader.dataset)\n",
        "#         critic_loss /= len(test_loader.dataset)\n",
        "#         print(\n",
        "#             \"\\nTest set: Average actor loss: {:.4f}, Average critic loss: {:.4f}\\n\".format(\n",
        "#                 actor_loss.item(), critic_loss.item()\n",
        "#             )\n",
        "#         )\n",
        "\n",
        "#     # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "#     # and testing\n",
        "#     train_loader = th.utils.data.DataLoader(\n",
        "#         dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "#     )\n",
        "#     test_loader = th.utils.data.DataLoader(\n",
        "#         dataset=test_expert_dataset,\n",
        "#         batch_size=test_batch_size,\n",
        "#         shuffle=True,\n",
        "#         **kwargs,\n",
        "#     )\n",
        "\n",
        "#     # Define an Optimizer and a learning rate schedule.\n",
        "#     actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "#     critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "#     # Now we are finally ready to train the policy model.\n",
        "#     for epoch in range(1, epochs + 1):\n",
        "#         train(actor, critic, device, train_loader, actor_optimizer, critic_optimizer)\n",
        "#         test(actor, critic, device, test_loader)\n",
        "\n",
        "#     # Implant the trained policy network back into the RL student agent\n",
        "#     student.policy.actor = actor\n",
        "#     student.policy.critic = critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretrain_agent(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=1000,\n",
        "    learning_rate=0.001,\n",
        "    log_interval=100,\n",
        "    no_cuda=False,\n",
        "    seed=1,\n",
        "    test_batch_size=64,\n",
        "):\n",
        "    use_cuda = not no_cuda and th.cuda.is_available()\n",
        "    th.manual_seed(seed)\n",
        "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    # Extract initial policy\n",
        "    actor = student.policy.actor.to(device)\n",
        "    critic = student.policy.critic.to(device)\n",
        "\n",
        "    def train(actor, critic, device, train_loader, actor_optimizer, critic_optimizer):\n",
        "        actor.train()\n",
        "        critic.train()\n",
        "\n",
        "        for batch_idx, (data, target_action, target_return) in enumerate(train_loader):\n",
        "            data, target_action, target_return = (\n",
        "                data.to(device),\n",
        "                target_action.to(device),\n",
        "                target_return.to(device),\n",
        "            )\n",
        "\n",
        "            target_action = target_action.float()\n",
        "            target_return = target_return.float()\n",
        "\n",
        "            action = actor(data)\n",
        "            actor_loss = F.mse_loss(action, target_action)\n",
        "\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            current_returns = critic(data, target_action)\n",
        "            critic_loss = sum(\n",
        "                F.mse_loss(current_return, target_return)\n",
        "                for current_return in current_returns\n",
        "            )\n",
        "\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            critic_optimizer.step()\n",
        "\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(\n",
        "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tActor Loss: {:.6f}\\tCritic Loss: {:.6f}\".format(\n",
        "                        epoch,\n",
        "                        batch_idx * len(data),\n",
        "                        len(train_loader.dataset),\n",
        "                        100.0 * batch_idx / len(train_loader),\n",
        "                        actor_loss.item(),\n",
        "                        critic_loss.item(),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def test(actor, critic, device, test_loader):\n",
        "        actor.eval()\n",
        "        critic.eval()\n",
        "        actor_loss = 0\n",
        "        critic_loss = 0\n",
        "        with th.no_grad():\n",
        "            for data, target_action, target_return in test_loader:\n",
        "                data, target_action, target_return = (\n",
        "                    data.to(device),\n",
        "                    target_action.to(device),\n",
        "                    target_return.to(device),\n",
        "                )\n",
        "\n",
        "                target_action = target_action.float()\n",
        "                target_return = target_return.float()\n",
        "\n",
        "                action = actor(data)\n",
        "                actor_loss += F.mse_loss(action, target_action)\n",
        "                current_returns = critic(data, target_action)\n",
        "                critic_loss += sum(\n",
        "                    F.mse_loss(current_return, target_return)\n",
        "                    for current_return in current_returns\n",
        "                )\n",
        "\n",
        "        actor_loss /= len(test_loader.dataset)\n",
        "        critic_loss /= len(test_loader.dataset)\n",
        "        print(\n",
        "            \"\\nTest set: Average actor loss: {:.4f}, Average critic loss: {:.4f}\\n\".format(\n",
        "                actor_loss.item(), critic_loss.item()\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "    # and testing\n",
        "    train_loader = th.utils.data.DataLoader(\n",
        "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "    )\n",
        "    test_loader = th.utils.data.DataLoader(\n",
        "        dataset=test_expert_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    # Define an Optimizer and a learning rate schedule.\n",
        "    actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "    critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Now we are finally ready to train the policy model.\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(actor, critic, device, train_loader, actor_optimizer, critic_optimizer)\n",
        "        test(actor, critic, device, test_loader)\n",
        "\n",
        "    # Implant the trained policy network back into the RL student agent\n",
        "    student.policy.actor = actor\n",
        "    student.policy.critic = critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\QuangHung\\Autonomous-Driving_\\pretraining.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m         \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m log_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m             \u001b[39mprint\u001b[39m(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTrain Epoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.0f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m)]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mActor Loss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mCritic Loss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m                     epoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m                 )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m             )\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m train(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     student,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     log_interval\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     no_cuda\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m )\n",
            "\u001b[1;32mc:\\QuangHung\\Autonomous-Driving_\\pretraining.ipynb Cell 27\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Optimize the critics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m student\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m critic_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m student\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving_/pretraining.ipynb#X46sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# Delayed policy updates\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from stable_baselines3.common.utils import polyak_update\n",
        "\n",
        "\n",
        "def train(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=1000,\n",
        "    log_interval=100,\n",
        "    no_cuda=False,\n",
        "    seed=42,\n",
        ") -> None:\n",
        "    use_cuda = not no_cuda and th.cuda.is_available()\n",
        "    th.manual_seed(seed)\n",
        "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {\"num_workers\": 0, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "    # and testing\n",
        "    train_loader = th.utils.data.DataLoader(\n",
        "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "    )\n",
        "\n",
        "    student.policy.to(device)\n",
        "\n",
        "    # Switch to train mode (this affects batch norm / dropout)\n",
        "    student.policy.set_training_mode(True)\n",
        "\n",
        "    # Update learning rate according to lr schedule\n",
        "    # student._update_learning_rate([student.actor.optimizer, student.critic.optimizer])\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, (\n",
        "            observations,\n",
        "            actions,\n",
        "            rewards,\n",
        "            dones,\n",
        "            next_observations,\n",
        "        ) in enumerate(train_loader):\n",
        "            observations, actions, rewards, dones, next_observations = (\n",
        "                observations.to(device),\n",
        "                actions.to(device),\n",
        "                rewards.to(device),\n",
        "                dones.to(device),\n",
        "                next_observations.to(device),\n",
        "            )\n",
        "            student._n_updates += 1\n",
        "\n",
        "            with th.no_grad():\n",
        "                # Select action according to policy and add clipped noise\n",
        "                next_actions = student.actor_target(next_observations)\n",
        "\n",
        "                # Compute the next Q-values: min over all critics targets\n",
        "                next_q_values = th.cat(\n",
        "                    student.critic_target(next_observations, next_actions),\n",
        "                    dim=1,\n",
        "                )\n",
        "                next_q_values, _ = th.min(next_q_values, dim=1, keepdim=True)\n",
        "                target_q_values = rewards + (1 - dones) * student.gamma * next_q_values\n",
        "\n",
        "            # Get current Q-values estimates for each critic network\n",
        "            current_q_values = student.critic(observations, actions)\n",
        "\n",
        "            # Compute critic loss\n",
        "            critic_loss = sum(\n",
        "                F.mse_loss(current_q, target_q_values) for current_q in current_q_values\n",
        "            )\n",
        "            assert isinstance(critic_loss, th.Tensor)\n",
        "\n",
        "            # Optimize the critics\n",
        "            student.critic.optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            student.critic.optimizer.step()\n",
        "\n",
        "            # Delayed policy updates\n",
        "            if student._n_updates % student.policy_delay == 0:\n",
        "                # Compute actor loss\n",
        "                actor_loss = -student.critic.q1_forward(\n",
        "                    observations, student.actor(observations)\n",
        "                ).mean()\n",
        "\n",
        "                # Optimize the actor\n",
        "                student.actor.optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                student.actor.optimizer.step()\n",
        "\n",
        "                polyak_update(\n",
        "                    student.critic.parameters(),\n",
        "                    student.critic_target.parameters(),\n",
        "                    student.tau,\n",
        "                )\n",
        "                polyak_update(\n",
        "                    student.actor.parameters(),\n",
        "                    student.actor_target.parameters(),\n",
        "                    student.tau,\n",
        "                )\n",
        "                # Copy running stats, see GH issue #996\n",
        "                polyak_update(\n",
        "                    student.critic_batch_norm_stats,\n",
        "                    student.critic_batch_norm_stats_target,\n",
        "                    1.0,\n",
        "                )\n",
        "                polyak_update(\n",
        "                    student.actor_batch_norm_stats,\n",
        "                    student.actor_batch_norm_stats_target,\n",
        "                    1.0,\n",
        "                )\n",
        "    \n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tActor Loss: {:.6f}\\tCritic Loss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(dones),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    actor_loss.item(),\n",
        "                    critic_loss.item(),\n",
        "                )\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEP6i0hEu_R"
      },
      "source": [
        "Evaluate the agent before pretraining, it should be random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_7kvYIneEui8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = -1344.4609348239378 +/- 59.453153859642484\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgduZAbF8ao9"
      },
      "source": [
        "\n",
        "\n",
        "Having defined the training procedure we can now run the pretraining!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "eI1EFFnW8ao-"
      },
      "outputs": [],
      "source": [
        "train(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=100,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=42,\n",
        ")\n",
        "student.save(\"student\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3Q5Jm58apE"
      },
      "source": [
        "\n",
        "\n",
        "Finally, let us test how well our RL agent student learned to mimic the behavior of the expert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKZ8O--m8apF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean reward = -147.9170643418096 +/- 69.77307351132359\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwutsYK5Ml9t"
      },
      "outputs": [],
      "source": [
        "student.save(\"student\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(env_id, render_mode=\"human\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -435     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 23       |\n",
            "|    time_elapsed    | 34       |\n",
            "|    total_timesteps | 800      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 7.43     |\n",
            "|    critic_loss     | 40.8     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 600      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -811     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 70       |\n",
            "|    total_timesteps | 1600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 18.4     |\n",
            "|    critic_loss     | 32.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -943     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 106      |\n",
            "|    total_timesteps | 2400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 26.2     |\n",
            "|    critic_loss     | 32.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2200     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -957     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 142      |\n",
            "|    total_timesteps | 3200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 33.2     |\n",
            "|    critic_loss     | 42.9     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -968     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 178      |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 38.7     |\n",
            "|    critic_loss     | 52.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3800     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -927     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 214      |\n",
            "|    total_timesteps | 4800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.3     |\n",
            "|    critic_loss     | 65.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 4600     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -909     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 250      |\n",
            "|    total_timesteps | 5600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 44.5     |\n",
            "|    critic_loss     | 74.1     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -824     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 286      |\n",
            "|    total_timesteps | 6400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 45.5     |\n",
            "|    critic_loss     | 76       |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 6200     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -836     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 322      |\n",
            "|    total_timesteps | 7200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 47.7     |\n",
            "|    critic_loss     | 90.6     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -880     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 358      |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 52.4     |\n",
            "|    critic_loss     | 102      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7800     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -905     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 394      |\n",
            "|    total_timesteps | 8800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57       |\n",
            "|    critic_loss     | 122      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8600     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -840     |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 430      |\n",
            "|    total_timesteps | 9600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57.5     |\n",
            "|    critic_loss     | 131      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9400     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -795     |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 466      |\n",
            "|    total_timesteps | 10400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 55.9     |\n",
            "|    critic_loss     | 123      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 10200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -750     |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 503      |\n",
            "|    total_timesteps | 11200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 55.2     |\n",
            "|    critic_loss     | 129      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -713     |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 539      |\n",
            "|    total_timesteps | 12000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 53.3     |\n",
            "|    critic_loss     | 128      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -679     |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 575      |\n",
            "|    total_timesteps | 12800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 52.3     |\n",
            "|    critic_loss     | 126      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 12600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -647     |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 611      |\n",
            "|    total_timesteps | 13600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 50.4     |\n",
            "|    critic_loss     | 118      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 13400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -617     |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 648      |\n",
            "|    total_timesteps | 14400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 48.9     |\n",
            "|    critic_loss     | 121      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 14200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -594     |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 684      |\n",
            "|    total_timesteps | 15200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 47.6     |\n",
            "|    critic_loss     | 130      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -568     |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 720      |\n",
            "|    total_timesteps | 16000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 45.9     |\n",
            "|    critic_loss     | 125      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -549     |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 756      |\n",
            "|    total_timesteps | 16800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 45.7     |\n",
            "|    critic_loss     | 128      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 16600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -533     |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 793      |\n",
            "|    total_timesteps | 17600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 43       |\n",
            "|    critic_loss     | 143      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 17400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -521     |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 829      |\n",
            "|    total_timesteps | 18400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.9     |\n",
            "|    critic_loss     | 121      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 18200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -505     |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 864      |\n",
            "|    total_timesteps | 19200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 41.7     |\n",
            "|    critic_loss     | 123      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -502     |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 900      |\n",
            "|    total_timesteps | 20000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.5     |\n",
            "|    critic_loss     | 124      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -491     |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 936      |\n",
            "|    total_timesteps | 20800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 40.9     |\n",
            "|    critic_loss     | 124      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 20600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -453     |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 973      |\n",
            "|    total_timesteps | 21600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 40.8     |\n",
            "|    critic_loss     | 108      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 21400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -411     |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1009     |\n",
            "|    total_timesteps | 22400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 39.5     |\n",
            "|    critic_loss     | 106      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 22200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -378     |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1045     |\n",
            "|    total_timesteps | 23200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 39.1     |\n",
            "|    critic_loss     | 107      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -346     |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1081     |\n",
            "|    total_timesteps | 24000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 37.9     |\n",
            "|    critic_loss     | 111      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23800    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -325     |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1117     |\n",
            "|    total_timesteps | 24800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 37.6     |\n",
            "|    critic_loss     | 104      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 24600    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -297     |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1153     |\n",
            "|    total_timesteps | 25600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 36.3     |\n",
            "|    critic_loss     | 98.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 25400    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -297     |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1190     |\n",
            "|    total_timesteps | 26400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 36       |\n",
            "|    critic_loss     | 106      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 26200    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -263     |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 22       |\n",
            "|    time_elapsed    | 1226     |\n",
            "|    total_timesteps | 27200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 36       |\n",
            "|    critic_loss     | 97.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27000    |\n",
            "---------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\QuangHung\\Autonomous-Driving\\pretraining.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving/pretraining.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m student \u001b[39m=\u001b[39m TD3\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mtd3_expert\u001b[39m\u001b[39m\"\u001b[39m, env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving/pretraining.ipynb#X44sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m student\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e5\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/QuangHung/Autonomous-Driving/pretraining.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m student\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mtd3_expert\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[39mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    223\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    224\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    225\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    226\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    227\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    228\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    229\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:333\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    332\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 333\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    335\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    337\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:179\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    176\u001b[0m     target_q_values \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mrewards \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m replay_data\u001b[39m.\u001b[39mdones) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_q_values\n\u001b[0;32m    178\u001b[0m \u001b[39m# Get current Q-values estimates for each critic network\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m current_q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic(replay_data\u001b[39m.\u001b[39;49mobservations, replay_data\u001b[39m.\u001b[39;49mactions)\n\u001b[0;32m    181\u001b[0m \u001b[39m# Compute critic loss\u001b[39;00m\n\u001b[0;32m    182\u001b[0m critic_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(F\u001b[39m.\u001b[39mmse_loss(current_q, target_q_values) \u001b[39mfor\u001b[39;00m current_q \u001b[39min\u001b[39;00m current_q_values)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\policies.py:937\u001b[0m, in \u001b[0;36mContinuousCritic.forward\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    935\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor)\n\u001b[0;32m    936\u001b[0m qvalue_input \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([features, actions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 937\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(q_net(qvalue_input) \u001b[39mfor\u001b[39;49;00m q_net \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_networks)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\stable_baselines3\\common\\policies.py:937\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    935\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor)\n\u001b[0;32m    936\u001b[0m qvalue_input \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([features, actions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 937\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(q_net(qvalue_input) \u001b[39mfor\u001b[39;00m q_net \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_networks)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\carla\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "student = TD3.load(\"student\", env)\n",
        "student.learn(total_timesteps=1e5)\n",
        "student.save(\"student\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "pretraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
